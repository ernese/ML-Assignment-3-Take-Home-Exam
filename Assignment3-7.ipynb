{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ernese/ML-Assignment-3-Take-Home-Exam/blob/master/Assignment3-7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZoGL_3iEGsp",
        "colab_type": "text"
      },
      "source": [
        "##Introduction\n",
        "Ensemble methods have helped machine learning to be more robust and have more accurate results by combining predictions of a number of classifiers to produce a single classifier. The idea of ensemble methods came to be because base models usually had high bias, noise, and have too much variance thus not having the most ideal performance. Ensemble methods reduce the bias, noise, and variance creating a strong learner to achieve a better performance as compared to a single base model. \n",
        "\n",
        "In this assignment I will focus on two well-known and highly used ensemble methods, these are Bagging and Boosting. One may be more accurate and have a higher performance as compared to the other, but this varies depending on the given data and circumstances. These ensemble methods, just like any other, are mainly used to improve and acquire better results as compared to using simple individual classifiers.\n",
        "\n",
        "In the following sections of this assignment I will be expounding more on both Bagging and Boosting, show the advantages and disadvantages of both, actual results using decision trees an neural networks, and propose a solution or a new method that could or potentially outperform both Bagging and Boosting.\n",
        "\n",
        "##Bagging and Boosting\n",
        "In order to be able to apply Bagging or Boosting one must first select a base model algorithm. Both ensemble methods rely on resampling techniques to obtain different training sets for any classifier chosen to be used. Both Bagging and Boosting combine several weak learning algorithms to create one strong learner for optimized performance and results. The ideal type of model when combining several classifiers is only when these classifiers are highly correct classifiers and that they disagree with one another. If we use several identical classifiers these ensembles methods will be inefficient and of no use.\n",
        "\n",
        "Bagging is proven to always be more accurate than any standard classifier, this is due to the fact that Bagging allows ensemble models to obtain a lower variance than its components. That is why base models with a low bias and high variance and unstable learning algorithms are best suited for Bagging. Bagging resamples the training set with a replacement, some results may be presented a number of times while others will be totally excluded, this is repeated a number of times to create several models and these models will be trained in parallel. Due to this, a classifier trained on a specific training set may attain a higher test-error than the classifier running all of the given data. \n",
        "\n",
        "Unlike Bagging, Boosting is mainly dependent on the characteristics and performance of the classifier being examined, it incorporates different methods and these methods produce a series of classifiers. Boosting usually combines weak learning algorithms to make strong learners, and instead of learning them in a parallel method as Bagging does, it learns them in a sequential method. With Boosting the goal is to adapt to the weaknesses of the previous classifiers and also utilize weighted averages, this is done by selecting samples that are inaccurately predicted by the previous classifiers, so additional weight is preset to these samples that were inaccurately predicted. That’s why a model with a lower variance but high bias is the ideal type of model to work with Boosting. \n",
        "\n",
        "##Advantages and Disadvantages\n",
        "Bagging and Boosting as stated in the section previous to this prove to be effective and valuable ensemble methods that increase accuracy and performance. Though this may be true, one must always put into consideration the use of any method by looking at the advantages and disadvantages. I will be stating a few of them in this section.\n",
        "\n",
        "Bagging is diverse and is proven to give better results as compared to standard classifiers. It also provides a more stable and accurate method for machine learning especially for classification and regression. The nature of bagging reduces the variance and thus avoids the problem of overfitting. As for Boosting, not only does it reduce errors, but also has a variation of forms that can further enhance it. Methods such as Arcing and Ada-Boosting are just a couple of them, these methods recalculate probabilities and are improved methods of Boosting. Boosting also supports different loss functions and also works well with interactions.\n",
        "\n",
        "Depending on the data sets Bagging can mildly degrade the performance of stable classifiers, also a loss of interpretability is possible. Problems such as classifiers having a high bias is also highly likely if it isn’t modeled properly. With a higher accuracy comes with a high computational cost, so the use of Bagging should be carefully thought about depending on the desired results and data set. The same as Bagging computational cost for Boosting is high as well, it involves careful alteration of hyper-parameters. Boosting is also mainly dependent on the properties of the dataset; thus, some datasets will be hard to implement. The major disadvantage of Boosting is that due to its sensitivity to noisy datasets, the performance decreases.\n",
        "\n",
        "##New Method - RotBoost\n",
        "After having briefly discussed both Bagging and Boosting, looking at its different characteristics and how both operate as ensemble methods, as well as showing a few advantages and disadvantages for both, we can see that there are still some issues that hinders performance or can be tweaked upon for improvement. Doing research for ways to improve or outperform mentioned ensemble methods, I stumbled upon an article on an ensemble method that combines Rotation Forest and AdaBoost which is called RotBoost.  Rotation Forest and AdaBoost being highly recognized ensemble methods and both having been proven to have a high performance rate with respect to other ensemble members, I found that the introduced ensemble method RotBoost from the article RotBoost: A technique for combining Rotation Forest and AdaBoost being highly informative and gave me a better insight for a method that can outperform both Bagging and Boosting.\n",
        "\n",
        "Rotation Forest follows the design of Bagging in which models are trained in a parallel manner, with this it also simultaneously allows diversity and is followed through by feature extraction. It is proven that Rotation Forest achieves better performance than Bagging, but again depending on the given dataset since size is important. In Rotation Forest each training set’s basis is modeled by applying PCA, this rotates the original attribute axes and reduces the dimension of data.\n",
        "\n",
        "AdaBoost, just like any boosting method follows a sequential algorithm, but due to the specific modifications made thru AdaBoost, this method provides an increased prediction accuracy as compared to other of its counterparts. All instances of the training are maintained equal and have the same importance, in which the end result is derived from the weighted voting of the outputs of each classifier.\n",
        "\n",
        "Having briefly explained both Rotation Forest and AdaBoost, we can imagine the potential and ability of RotBoost being able to outperform both Bagging and Boosting. Just like any ensemble method, it is important for the classifiers to have a high accuracy and at the same time disagree as much as possible. Thus, it is possible that prediction accuracy for different types of ensemble methods to improve, under the condition that diversity increases, and individual errors are unchanged. The article mentions that using different notions from Rotation Forest and AdaBoost, increasing diversity is possible without affecting individual error as much.\n",
        "\n",
        "With the impression of ensemble methods being constructed given a simple individual classifier by use of permutated training sets, the article described how RotBoost came to be by combining Rotation Forest and AdaBoost was made possible. With the concept of bias-variance decomposition, RotBoost accomplishes what Rotation Forest manages to do and that is to simultaneously allows diversity in which bias and variance are reduced, this proves that RotBoost outperforms other ensemble methods, but not only that RotBoost also has the advantage over AdaBoost because of its parallel execution. RotBoost, being the result of combining the two powerful ensemble methods has shown that it manages to generate significantly lower prediction errors than the two and at the same time also achieves a better performance as compared to Bagging and MultiBoost.\n",
        "\n",
        "The results shown in the article prove the RotBoost achieves lower error than Rotation Forest and AdaBoost and this is supported by RotBoost being able to successfully combine notions from both. But even though the results are very promising, the article also mentions a few problems in RotBoost itself that brings up a few concerns. The concerns following the article; RotBoost was only tested by using a single classification tree, so the question of how it performs with other base algorithms such as neural networks, will the results be different, and can it solve regression problems? But even with these concerns, we can see that ensemble methods can continue to improve and at the same time evolve by combining different ensemble classifiers or method with one another for as long as we follow the main idea of ensemble methods.\n",
        "\n",
        "##Conclusion\n",
        "\n",
        "\n",
        "##References\n",
        "\n",
        "Opitz, D. and Maclin, R., 1999. Popular ensemble methods: An empirical study. *Journal of artificial intelligence research*, 11, pp.169-198.\n",
        "\n",
        "Packt 2018. *Rotation Forest - A Classifier Ensemble Based on Feature Extraction*, viewed 03 October 2019, <https://hub.packtpub.com/rotation-forest-classifier-ensemble-based-feature-extraction/>\n",
        "\n",
        "Xia, J., Du, P., He, X. and Chanussot, J., 2013. Hyperspectral remote sensing image classification based on rotation forest. *IEEE Geoscience and Remote Sensing Letters*, 11(1), pp.239-243.\n",
        "\n",
        "Zhang, C.X, Zhang, J.S., 2007, *RotBoost: A technique for combining Rotation Forest and AdaBoost*, viewed 05 October 2019, <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.1518&rep=rep1&type=pdf>\n",
        "\n",
        "Zhou, Z.H., 2012. *Ensemble methods: foundations and algorithms*. Chapman and Hall/CRC.\n",
        "\n",
        "##Plain text link to my GitHub\n"
      ]
    }
  ]
}