{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ernese/ML-Assignment-3-Take-Home-Exam/blob/master/Assignment3-6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZoGL_3iEGsp",
        "colab_type": "text"
      },
      "source": [
        "##Introduction\n",
        "Ensemble methods have helped machine learning to be more robust and have more accurate results by combining predictions of a number of classifiers to produce a single classifier. The idea of ensemble methods came to be because base models usually had high bias, noise, and have too much variance thus not having the most ideal performance. Ensemble methods reduce the bias, noise, and variance creating a strong learner to achieve a better performance as compared to a single base model. \n",
        "\n",
        "In this assignment I will focus on two well-known and highly used ensemble methods, these are Bagging and Boosting. One may be more accurate and have a higher performance as compared to the other, but this varies depending on the given data and circumstances. These ensemble methods, just like any other, are mainly used to improve and acquire better results as compared to using simple individual classifiers.\n",
        "\n",
        "In the following sections of this assignment I will be expounding more on both Bagging and Boosting, show the advantages and disadvantages of both, actual results using decision trees an neural networks, and propose a solution or new method/s that could or potentially outperform both Bagging and Boosting.\n",
        "\n",
        "##Bagging and Boosting\n",
        "In order to be able to apply Bagging or Boosting one must first select a base model algorithm. Both ensemble methods rely on resampling techniques to obtain different training sets for any classifier chosen to be used. Both Bagging and Boosting combine several weak learning algorithms to create one strong learner for optimized performance and results. The ideal type of model when combining several classifiers is only when these classifiers are highly correct classifiers and that they disagree with one another. If we use several identical classifiers these ensembles methods will be inefficient and of no use.\n",
        "\n",
        "Bagging is proven to always be more accurate than any standard classifier, this is due to the fact that Bagging allows ensemble models to obtain a lower variance than its components. That is why base models with a low bias and high variance and unstable learning algorithms are best suited for Bagging. Bagging resamples the training set with a replacement, some results may be presented a number of times while others will be totally excluded, this is repeated a number of times to create several models and these models will be trained in parallel. Due to this, a classifier trained on a specific training set may attain a higher test-error than the classifier running all of the given data. \n",
        "\n",
        "Unlike Bagging, Boosting is mainly dependent on the characteristics and performance of the classifier being examined, it incorporates different methods and these methods produce a series of classifiers. Boosting usually combines weak learning algorithms to make strong learners, and instead of learning them in a parallel method as Bagging does, it learns them in a sequential method. With Boosting the goal is to adapt to the weaknesses of the previous classifiers and also utilize weighted averages, this is done by selecting samples that are inaccurately predicted by the previous classifiers, so additional weight is preset to these samples that were inaccurately predicted. That’s why a model with a lower variance but high bias is the ideal type of model to work with Boosting. \n",
        "\n",
        "##Advantages and Disadvantages\n",
        "Bagging and Boosting as stated in the section previous to this prove to be effective and valuable ensemble methods that increase accuracy and performance. Though this may be true, one must always put into consideration the use of any method by looking at the advantages and disadvantages. I will be stating a few of them in this section.\n",
        "\n",
        "Bagging is diverse and is proven to give better results as compared to standard classifiers. It also provides a more stable and accurate method for machine learning especially for classification and regression. The nature of bagging reduces the variance and thus avoids the problem of overfitting. As for Boosting, not only does it reduce errors, but also has a variation of forms that can further enhance it. Methods such as Arcing and Ada-Boosting are just a couple of them, these methods recalculate probabilities and are improved methods of Boosting. Boosting also supports different loss functions and also works well with interactions.\n",
        "\n",
        "Depending on the data sets Bagging can mildly degrade the performance of stable classifiers, also a loss of interpretability is possible. Problems such as classifiers having a high bias is also highly likely if it isn’t modeled properly. With a higher accuracy comes with a high computational cost, so the use of Bagging should be carefully thought about depending on the desired results and data set. The same as Bagging computational cost for Boosting is high as well, it involves careful alteration of hyper-parameters. Boosting is also mainly dependent on the properties of the dataset; thus, some datasets will be hard to implement. The major disadvantage of Boosting is that due to its sensitivity to noisy datasets, the performance decreases.\n",
        "\n",
        "##New Method - RotBoost\n",
        "After having briefly discussed both Bagging and Boosting, looking at its different characteristics and how both operate as ensemble methods, as well as showing a few advantages and disadvantages for both, we can see that there are still some issues that hinders performance or can be tweaked upon for improvement. Doing research for ways to improve or outperform mentioned ensemble methods, I stumbled upon an article on an ensemble method that combines Rotation Forest and AdaBoost which is called RotBoost.  Rotation Forest and AdaBoost being highly recognized ensemble methods and both having been proven to have a high performance rate with respect to other ensemble members, I found that the introduced ensemble method RotBoost from the article RotBoost: A technique for combining Rotation Forest and AdaBoost being highly informative and gave me a better insight for a method that can outperform both Bagging and Boosting.\n",
        "\n",
        "\n",
        "\n",
        "##Conclusion\n",
        "\n",
        "\n",
        "##References\n",
        "\n",
        "Opitz, D. and Maclin, R., 1999. Popular ensemble methods: An empirical study. *Journal of artificial intelligence research*, 11, pp.169-198.\n",
        "\n",
        "Packt 2018. *Rotation Forest - A Classifier Ensemble Based on Feature Extraction*, viewed 03 October 2019, <https://hub.packtpub.com/rotation-forest-classifier-ensemble-based-feature-extraction/>\n",
        "\n",
        "Xia, J., Du, P., He, X. and Chanussot, J., 2013. Hyperspectral remote sensing image classification based on rotation forest. *IEEE Geoscience and Remote Sensing Letters*, 11(1), pp.239-243.\n",
        "\n",
        "Zhang, C.X, Zhang, J.S., 2007, *RotBoost: A technique for combining Rotation Forest and AdaBoost*, viewed 05 October 2019, <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.1518&rep=rep1&type=pdf>\n",
        "\n",
        "Zhou, Z.H., 2012. *Ensemble methods: foundations and algorithms*. Chapman and Hall/CRC."
      ]
    }
  ]
}