{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ernese/ML-Assignment-3-Take-Home-Exam/blob/master/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZoGL_3iEGsp",
        "colab_type": "text"
      },
      "source": [
        "##Introduction\n",
        "Ensemble methods have helped machine learning to be more robust and have more accurate results by combining predictions of a number of classifiers to produce a single classifier. The idea of ensemble methods came to be because base models usually had high bias, noise, and have too much variance thus not having the most ideal performance. Ensemble methods reduce the bias, noise, and variance creating a strong learner to achieve a better performance as compared to a single base model. \n",
        "\n",
        "In this assignment I will focus on two well-known and highly used ensemble methods, these are Bagging and Boosting. One may be more accurate and have a higher performance as compared to the other, but this varies depending on the given data and circumstances. These ensemble methods, just like any other, are mainly used to improve and acquire better results as compared to using simple individual classifiers.\n",
        "\n",
        "In the following sections of this assignment I will be expounding more on both Bagging and Boosting, show the advantages and disadvantages of both, actual results using decision trees an neural networks, mention an issue/s using both ensemble methods, and propose a solution or new method/s that could or potentially outperform both Bagging and Boosting.\n",
        "\n",
        "##Bagging and Boosting\n",
        "In order to be able to apply Bagging or Boosting one must first select a base model algorithm. Both ensemble methods rely on resampling techniques to obtain different training sets for any classifier chosen to be used. Both Bagging and Boosting combine several weak learners to create one strong learner for optimized performance and results. The ideal type of model when combining several classifiers is only when these classifiers are highly correct classifiers and that they disagree with one another. If we use several identical calssifiers these ensembles methods will be inefficient and of no use.\n",
        "\n",
        "Bagging is proven to always be more accurate than any standard classifier, this is due to the fact that Bagging allows ensemble models to obtain a lower variance than its components. That is why base models with a low bias and high variance and unstable learning algorithms are best suited for Bagging. Bagging resamples the training set with a replacement, some results may be presented a number of times while others will be totally excluded, this is repeated a number of times to create several models and these models will be trained in parallel. Due to this, a classifier trained on a specific training set may attain a higher test-error than the classifier running all of the given data. \n",
        "\n",
        "Unlike Bagging, Boosting is mainly dependent on the characteristics and performance of the classifier being examined, it incorporates different methods and these methods produce a series of classifiers. Boosting usually combines weak learners to make strong learners, and instead of learning them in a parallel method as Bagging does, it learns them in a sequential method. With Boosting the goal is to adapt to the weaknesses of the previous classifiers and also utilize weighted averages, this is done by selecting samples that are inaccurately predicted by the previous classifiers, so additional weight is preset to these samples that were inaccurately predicted. Thatâ€™s why a model with a lower variance but high bias is the ideal type of model to work with Boosting. \n",
        "\n",
        "##Bagging and Boosting using Decision Trees and Neural Networks\n",
        "\n",
        "\n",
        "##Advantages and Disadvantages\n",
        "\n",
        "\n",
        "##Issues with existing method\n",
        "\n",
        "\n",
        "##Proposal and explanation of new method\n",
        "\n",
        "\n",
        "##Conclusion\n",
        "\n",
        "##References\n",
        "\n",
        "Opitz, D. and Maclin, R., 1999. Popular ensemble methods: An empirical study. *Journal of artificial intelligence research*, 11, pp.169-198.\n",
        "\n",
        "Packt 2018. *Rotation Forest - A Classifier Ensemble Based on Feature Extraction*, viewed 03 October 2019, <https://hub.packtpub.com/rotation-forest-classifier-ensemble-based-feature-extraction/>\n",
        "\n",
        "Xia, J., Du, P., He, X. and Chanussot, J., 2013. Hyperspectral remote sensing image classification based on rotation forest. *IEEE Geoscience and Remote Sensing Letters*, 11(1), pp.239-243.\n",
        "\n",
        "Zhang, C.X, Zhang, J.S., 2007, *RotBoost: A technique for combining Rotation Forest and AdaBoost*, viewed 05 October 2019, <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.1518&rep=rep1&type=pdf>\n",
        "\n",
        "Zhou, Z.H., 2012. *Ensemble methods: foundations and algorithms*. Chapman and Hall/CRC."
      ]
    }
  ]
}